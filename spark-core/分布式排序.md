# Spark排序实现原理

* 排序的算子：
```scala
  /**
   * Return this RDD sorted by the given key function.
   */
  def sortBy[K](
      f: (T) => K,
      ascending: Boolean = true,
      numPartitions: Int = this.partitions.length)
      (implicit ord: Ordering[K], ctag: ClassTag[K]): RDD[T] = withScope {
    this.keyBy[K](f)
        .sortByKey(ascending, numPartitions)
        .values
  }

```

1、先调用f:(T)=>K 生成新的key
2、再调用sortByKey排序

* sortByKey排序原理  
算子：
```scala
  /**
   * Sort the RDD by key, so that each partition contains a sorted range of the elements. Calling
   * `collect` or `save` on the resulting RDD will return or output an ordered list of records
   * (in the `save` case, they will be written to multiple `part-X` files in the filesystem, in
   * order of the keys).
   */
  // TODO: this currently doesn't work on P other than Tuple2!
  def sortByKey(ascending: Boolean = true, numPartitions: Int = self.partitions.length)
      : RDD[(K, V)] = self.withScope
  {
    val part = new RangePartitioner(numPartitions, self, ascending)
    new ShuffledRDD[K, V, V](self, part)
      .setKeyOrdering(if (ascending) ordering else ordering.reverse)
  }
```

**关键点在：RangePartitioner。RangePartitioner把数据从小到大分布到不同partition.
shuffle后partition之间的数据是整体有序的。**

比如1-1000排序，10个partition  

1. 第一个partition为[1,100]  
2. 第二个partition为[101,200]  
3. 第三个partition为[201,300]
4. 第四个partition为[301,400]
5. 第五个partition为[401,500]
6. 第六个partition为[501,600]
7. ...

对比下Mysql这种关系型数据库的排序，一般是批量排序，写磁盘，再归并
Spark采用RangePartitioner,由于partition的数据是整体有序的，省去了归并的过程。
并且在海量数据的情况下，对排序后的数据进行归并，压力会都集中在Driver上，是不可能实现的。


* 那么在大数据量下怎么决定区间该怎么划分？采样。
```scala
      // This is the sample size we need to have roughly balanced output partitions, capped at 1M.
      // Cast to double to avoid overflowing ints or longs
      val sampleSize = math.min(samplePointsPerPartitionHint.toDouble * partitions, 1e6)
      // Assume the input partitions are roughly balanced and over-sample a little bit.
      val sampleSizePerPartition = math.ceil(3.0 * sampleSize / rdd.partitions.length).toInt
      val (numItems, sketched) = RangePartitioner.sketch(rdd.map(_._1), sampleSizePerPartition)
```

1. 首先确定要采用的数据大小，min(20* 用户自定义的range partitions个数,1M)，最大为1M。
2. 初步确定每个partition采用数据的大小。3* 采用数据的大小 / 该RDD的partition大小。注意。决定采样数据大小的是用户定义的range partition长度，决定每个分区采样的大小是range前的partition长度。
3. 用sketch函数获取rdd的总item个数，和(parttion,partition Sample)


* sketch函数里面调用了mapPartition对每个partition进行采样。  

```scala
  /**
   * Sketches the input RDD via reservoir sampling on each partition.
   *
   * @param rdd the input RDD to sketch
   * @param sampleSizePerPartition max sample size per partition
   * @return (total number of items, an array of (partitionId, number of items, sample))
   */
  def sketch[K : ClassTag](
      rdd: RDD[K],
      sampleSizePerPartition: Int): (Long, Array[(Int, Long, Array[K])]) = {
    val shift = rdd.id
    // val classTagK = classTag[K] // to avoid serializing the entire partitioner object
    val sketched = rdd.mapPartitionsWithIndex { (idx, iter) =>
      val seed = byteswap32(idx ^ (shift << 16))
      val (sample, n) = SamplingUtils.reservoirSampleAndCount(
        iter, sampleSizePerPartition, seed)
      Iterator((idx, n, sample))
    }.collect()
    val numItems = sketched.map(_._2).sum
    (numItems, sketched)
  }
```

* 采样使用了随机数生成器XORShiftRandom,性能要比java自带的随机数生成器好。

```scala
      var l = i.toLong
      val rand = new XORShiftRandom(seed)
      while (input.hasNext) {
        val item = input.next()
        l += 1
        // There are k elements in the reservoir, and the l-th element has been
        // consumed. It should be chosen with probability k/l. The expression
        // below is a random long chosen uniformly from [0,l)
        val replacementIndex = (rand.nextDouble() * l).toLong
        if (replacementIndex < k) {
          reservoir(replacementIndex.toInt) = item
        }
      }
```

# 问题
1. 采样是采用随机数生成器，而不是根据数据本身的分布情况，这个采样函数不是很完美.
2. 是否可能在极端情况下采样数据和实际数据偏差大，会导致数据倾斜很严重。


# 思考
再某些情况下，可以采用机器学习算法得出概率密码函数，再根据概率密度函数来自定义RangePartitioner，可能会更合理一点。
