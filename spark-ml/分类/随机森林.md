# 随机森林
随机森林的精要在于：随机采样特征，随机采样数据，生成多颗树。再根据信息增益、基尼系数等训练每颗树。
如果是分类，每颗树给出自己的结果，再根据树的权重和投票策略决定最后的预测结果。

## 连续特征离散化
```
        //总样本数除以/(切分的桶数+1)
        val stride: Double = numSamples.toDouble / (numSplits + 1)
        。。。
        // targetCount: target value for `currentCount`.
        // If `currentCount` is closest value to `targetCount`,
        // then current value is a split threshold.
        // After finding a split threshold, `targetCount` is added by stride.
        var targetCount = stride
        while (index < valueCounts.length) {
          val previousCount = currentCount
          currentCount += valueCounts(index)._2
          val previousGap = math.abs(previousCount - targetCount)
          val currentGap = math.abs(currentCount - targetCount)
          // If adding count of current value to currentCount
          // makes the gap between currentCount and targetCount smaller,
          // previous value is a split threshold.
          if (previousGap < currentGap) {
            splitsBuilder += (valueCounts(index - 1)._1 + valueCounts(index)._1) / 2.0
            targetCount += stride
          }
          index += 1
        }

        splitsBuilder.result()

```

核心代码是这两行
```
          val previousGap = math.abs(previousCount - targetCount)
          val currentGap = math.abs(currentCount - targetCount)
```

效果应该类似于等频。

### 问题
是否在数据预处理阶段，用mdlp或者卡方检验等方法离散化效果会更好点？

## 训练过程

```
    val trees = RandomForest
      .run(oldDataset, strategy, getNumTrees, getFeatureSubsetStrategy, getSeed, Some(instr))
      .map(_.asInstanceOf[DecisionTreeClassificationModel])
```

参数：
strategy:
```
  /** (private[ml]) Create a Strategy instance to use with the old API. */
  private[ml] def getOldStrategy(
      categoricalFeatures: Map[Int, Int],
      numClasses: Int,
      oldAlgo: OldAlgo.Algo,
      oldImpurity: OldImpurity,
      subsamplingRate: Double): OldStrategy = {
    val strategy = OldStrategy.defaultStrategy(oldAlgo)
    strategy.impurity = oldImpurity
    strategy.checkpointInterval = getCheckpointInterval
    strategy.maxBins = getMaxBins
    strategy.maxDepth = getMaxDepth
    strategy.maxMemoryInMB = getMaxMemoryInMB
    strategy.minInfoGain = getMinInfoGain
    strategy.minInstancesPerNode = getMinInstancesPerNode
    strategy.useNodeIdCache = getCacheNodeIds
    strategy.numClasses = numClasses
    strategy.categoricalFeaturesInfo = categoricalFeatures
    strategy.subsamplingRate = subsamplingRate
    strategy
  }
```

1. impurity 计算信息增益或者基尼系数
2. checkpointInterval checkpoint间隔
3. maxBins 连续特征的最大分箱数
4. maxDepth 决策树的最大深度
5. maxMemoryInMB 最大内存
6. minInfoGain 最小信息熵
7. minInstancesPerNode
8. useNodeIdCache
9. numClasses 分类的类别数量
10. categoricalFeaturesInfo
11. subsamplingRate 训练数据采样比例

下面看训练过程：
第一步，获取元数据，也就是采样比例，树的最大深度等等：
```
   val metadata =
      DecisionTreeMetadata.buildMetadata(retaggedInput, strategy, numTrees, featureSubsetStrategy)
```

第二步，找到连续变量的切分点

```
    val splits = findSplits(retaggedInput, metadata, seed)

```

第三步，生成treeRDD。（实际就是连续变量离散化的过程）
```
    val treeInput = TreePoint.convertToTreeRDD(retaggedInput, splits, metadata)

```

第四步，抽样一部分数据，并且cache到磁或者内存
```
    val baggedInput = BaggedPoint
      .convertToBaggedRDD(treeInput, strategy.subsamplingRate, numTrees, withReplacement, seed)
      .persist(StorageLevel.MEMORY_AND_DISK)
```

第五步，初始化根节点
```
    val nodeStack = new mutable.ArrayStack[(Int, LearningNode)]

    val rng = new Random()
    rng.setSeed(seed)

    // Allocate and queue root nodes.
    val topNodes = Array.fill[LearningNode](numTrees)(LearningNode.emptyNode(nodeIndex = 1))
    Range(0, numTrees).foreach(treeIndex => nodeStack.push((treeIndex, topNodes(treeIndex))))

    timer.stop("init")
```

第六步，每颗树抽样选择部分特征（**随机森林的第一次随机**）
```
      val featureSubset: Option[Array[Int]] = if (metadata.subsamplingFeatures) {
        Some(SamplingUtils.reservoirSampleAndCount(Range(0,
          metadata.numFeatures).iterator, metadata.numFeaturesPerNode, rng.nextLong())._1)
      } else {
        None
      }
```

第七步，训练随机森林的每颗树。

难点还是在分布式环境下，如何高效的计算每个特征的信息熵和基尼系数，进行特征切分。


