# 连续变量离散化

类：org.apache.spark.ml.feature.Bucketizer

方法：transform

```
    val splits = Array(Double.NegativeInfinity, -0.5, 0.0, 0.5, Double.PositiveInfinity)

    val data = Array(-999.9, -0.5, -0.3, 0.0, 0.2, 999.9)
    val dataFrame = spark.createDataFrame(data.map(Tuple1.apply)).toDF("features")

    val bucketizer = new Bucketizer()
      .setInputCol("features")
      .setOutputCol("bucketedFeatures")
      .setSplits(splits)

    // Transform original data into its bucket index.
    val bucketedData = bucketizer.transform(dataFrame)

```

工作流程:

1. 用户定义切分区间

2. 实例化Bucketizer

3. 指定要切分的列，切分后的列名，和切分区间

4. tranform函数里面通过java.util.binarySearch()方法进行二分查找。
binarySearch这个函数，如果查找的值在数组里面，则返回数组下标。
如果不在则返回-(low + 1) low为最后一次二分查找的区间左端点。
对binarySearch的返回结果做一次处理，可以实现区间查找。

```
      val idx = ju.Arrays.binarySearch(splits, feature)
      if (idx >= 0) {
        idx
      } else {
        val insertPos = -idx - 1
        if (insertPos == 0 || insertPos == splits.length) {
          throw new SparkException(s"Feature value $feature out of Bucketizer bounds" +
            s" [${splits.head}, ${splits.last}].  Check your features, or loosen " +
            s"the lower/upper bound constraints.")
        } else {
          insertPos - 1
        }
      }
```

