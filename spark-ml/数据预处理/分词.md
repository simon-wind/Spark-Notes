# 分词

Spark自带有一个简单的文本切割器：Tokenizer和RegexTokenizer

Tokenizer默认用空格分词，RegexTokenizer支持正则。只适用于全是英文的简单分词场景。

中文分词可以参考：

1. HanNlp  https://github.com/hankcs/HanLP
2. ansj  https://github.com/NLPchina/ansj_seg
3. 结巴分词  https://github.com/huaban/jieba-analysis



